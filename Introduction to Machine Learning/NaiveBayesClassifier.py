# Naive Bayes belongs to the family of classifiers, which are based on simple probabilistic models
# of how the data in each class might have been generated.

# Naive Bayes classifiers are called naive because informally, they make the simplifying assumption \
# that each feature of an instance is independent of all the others, given the class. Given the class here
# means that for each class.

# This lecture will focus on Gaussian Naive Bayes classifiers which assume features that are continuous or
# real-valued. During training, the Gaussian Naive Bayes Classifier estimates for each feature the mean and
# standard deviation of the feature value for each class.

# For prediction, the classifier compares the features of the example data point to be predicted with the
# feature statistics for each class and selects the class that best matches the data point.
# More specifically, the Gaussian Naive Bayes Classifier assumes that the data for each class was
# generated by a simple class specific Gaussian distribution.
# Predicting the class of a new data point corresponds mathematically to estimating the probability that
# each classes Gaussian distribution was most likely to have generated the data point. Classifier then
# picks the class that has the highest probability.
#  The centers of the Gaussian's correspond to the mean value of each feature for each class.

# There are two other types of Naive Bayes Classifier other than Gausiann which is basically used in text mining
# Bernoulli - Like we want to represent the absence or presense of any word in a text considering words
# as features
# Multinomial  uses a set of count base features each of which does account for how many times a particular
# feature such as a word is observed in training example like a document.


from sklearn.naive_bayes import GaussianNB
from adspy_shared_utilities import plot_class_regions_for_classifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification, make_blobs

X_C2, y_C2 = make_classification(n_samples = 100, n_features=2,
                                 n_redundant=0, n_informative=2,
                                 n_clusters_per_class=1, flip_y = 0.1,
                                 class_sep = 0.5, random_state=0)
X_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state=0)

nbclf = GaussianNB().fit(X_train, y_train)
plot_class_regions_for_classifier(nbclf, X_train, y_train, X_test, y_test,
                                  'Gaussian Naive Bayes classifier: Dataset 1')