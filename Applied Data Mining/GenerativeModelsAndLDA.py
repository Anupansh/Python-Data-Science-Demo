#  Unique model is the one where you have one topic distribution and you get words from there.
#  Mixture model is the one where we have the same document, generated by more than one topics. Some
#  of them represented with a higher proportion and others that are not

# LDA stands for Latent Dirichlet Allocation.
#  LDA is also a generative model and it creates its documents based on some notion of length of the document
#  , mixture of topics in that document and then, individual topics multinomial distributions.
# Like first specify what will be the length of document than what will be the distribution of topic like 40
# percent of words will be from genetic topic and than use multinomial to generate that


# ------------------------------------  Internet Definition --------------------------------------------------

# LDAâ€™s approach to topic modeling is it considers each document as a collection of topics in a certain proportion.
# And each topic as a collection of keywords, again, in a certain proportion. Once you provide the algorithm with the
# number of topics, all it does it to rearrange the topics distribution within the documents and keywords
# distribution within the topics to obtain a good composition of topic-keywords distribution.
# When I say topic, what is it actually and how it is represented?
# A topic is nothing but a collection of dominant keywords that are typical representatives. Just by looking at the
# keywords, you can identify what the topic is all about.

from nltk.book import *
from gensim import corpora, models
dictionary = corpora.Dictionary(text1) # Making ID-Word Dictioanry
corpus = [dictionary.doc2bow(doc) for doc in doc_set]
ldamodel = gensim.models.ldamodel.LdaModel (corpus, num_topics=4,
                                            id2word=dictionary, passes=50)
print(ldamodel.print_topics(num_topics=4, num_words=5))

# ----------------------------------------- Information Extraction ------------------------------------------

# There are a lot of unstructured information available on internet from which we can deduce useful deductions and
# fetch what's important
# Named entities are the ones which are feild of interest which differs from sentence to sentence like in RAju gifted
# Aasma . Raju and Aasma are first persons
# This builds on the named entity recognition task and the relation extraction and co-reference resolution

#  Entity Recognization - Entity of Interst - like PRP treats hairfall here PRP and hairfall are subjects of interst
# Relation - In the above example treats is the relation between two
# Co - refrence Resolution- Giving refrence to that particular entity. for ex PRP treats hairfall. It is one of the
# best treatment for it . Here in the second like first it represents PRP and second it represents hairfall i.e they
# are refrencing them